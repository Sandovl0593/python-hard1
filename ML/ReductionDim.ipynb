{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: Objetivo: Es una proyeccion ortogonal o transformacion lineal que busca maximizar la varianza de los datos proyectados\n",
    "def PCA(X: np.ndarray, k):\n",
    "    # Center the data\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Calculate the covariance matrix\n",
    "    cov = np.cov(X, rowvar=False)\n",
    "    \n",
    "    # Calculate the eigenvectors and eigenvalues\n",
    "    eigvals, eigvecs = np.linalg.eig(cov)\n",
    "    \n",
    "    # Sort the eigenvectors by decreasing eigenvalues\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    \n",
    "    # select the top k eigenvectors to form the principal matrix W\n",
    "    W = eigvecs[:, idx[:k]]\n",
    "    \n",
    "    # Project the data onto the principal components\n",
    "    Z = X @ W  # Z = XW\n",
    "    return W, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA: Maximiza la distancia entre las medias de las proyecciones de las clases y minimiza sus varianzas\n",
    "def LDA(X: np.ndarray, y: np.ndarray, C, k):\n",
    "    \"\"\"y: vector de etiquetas de las clases, C: numero de clases\"\"\"\n",
    "    m = X.shape[0], n = X.shape[1]  # X in R^{m x n}, Y in R^m\n",
    "\n",
    "    # compute the mean vectors for each class\n",
    "    mean_vectors = []\n",
    "    for c in range(C):\n",
    "        mean_vectors.append(np.mean(X[y == c], axis=0))\n",
    "    mean_vectors = np.array(mean_vectors)\n",
    "\n",
    "    # compute the overall mean\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "\n",
    "    # compute the between-class scatter matrix S_B\n",
    "    S_B = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for c in range(C):\n",
    "        n_c = X[y == c].shape[0]\n",
    "        S_B += n_c * (mean_vectors[c] - overall_mean) @ (mean_vectors[c] - overall_mean).T\n",
    "\n",
    "    # compute the within-class scatter matrix S_W\n",
    "    S_W = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for c in range(C):\n",
    "        S_W += np.cov(X[y == c], rowvar=False)  # covariance matrix = sum((X - mean) @ (X - mean).T) / (n - 1)\n",
    "\n",
    "    # compute the eigenvectors and eigenvalues of S_W^-1 S_B, set \\lambda as the eigenvalues and w as the eigenvectors\n",
    "    eigvals, eigvecs = np.linalg.eig(np.linalg.inv(S_W) @ S_B)\n",
    "\n",
    "    # select the top k eigenvectors corresponding to the k largest eigenvalues to form the principal matrix W\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    W = eigvecs[:, idx[:k]]\n",
    "\n",
    "    # project the data onto the principal components\n",
    "    Z = X @ W  # Z = XW\n",
    "\n",
    "    return W, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD: Objetivo: Descomponer una matriz en tres matrices, U, S y V, donde U y V son ortogonales y S es diagonal\n",
    "def SVD(X: np.ndarray):  # X in R^{m x n}\n",
    "    \n",
    "# easy to use the built-in function\n",
    "    U_l, Sigma_l, V_l = np.linalg.svd(X)  \n",
    "\n",
    "# intern process algorithm\n",
    "    # calculate the covariance matrix\n",
    "    C = X.T @ X\n",
    "\n",
    "    # calculate the eigenvectors and eigenvalues\n",
    "    eigvals, eigvecs = np.linalg.eig(C)\n",
    "\n",
    "    # sort the eigenvectors by decreasing eigenvalues\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "\n",
    "    # form the matrix V using the top n eigenvectors\n",
    "    V = eigvecs[:, idx]\n",
    "\n",
    "    # compute the singular values \\sigma as the square root of the eigenvalues of C\n",
    "    sigma = np.sqrt(eigvals[idx])\n",
    "    # form the matrix sigma with the singular values on the diagonal\n",
    "    Sigma = np.diag(sigma)\n",
    "\n",
    "    # compute the matrix U using the formula U = X @ V @ Sigma^-1\n",
    "    U = X @ V @ np.linalg.inv(Sigma)\n",
    "\n",
    "    return U, Sigma, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMMF: Objetivo: Descomponer una matriz en dos matrices no negativas, similar a la SVD\n",
    "def NMMF(X: np.ndarray, rank, max_iter=100, toler=0.001): # X in R^{m x n}\n",
    "    # NOTE: we use multiplicative update rules for non-negative matrix factorization\n",
    "\n",
    "    # init W and H\n",
    "    W = np.random.rand(X.shape[0], rank)\n",
    "    H = np.random.rand(rank, X.shape[1])\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # update W\n",
    "        W = W * ((X @ H.T) / (W @ H @ H.T))\n",
    "\n",
    "        # update H\n",
    "        H = H * ((W.T @ X) / (W.T @ W @ H))\n",
    "\n",
    "        # compute the error\n",
    "        error = np.linalg.norm(X - W @ H)\n",
    "        if error < toler:\n",
    "            break\n",
    "        \n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Projection: Objetivo: Proyectar los datos en un espacio de menor dimension, donde se preserva la distancia entre los puntos en el espacio original\n",
    "def RandomProjection(X: np.ndarray, target_dim):\n",
    "    # generate a random projection matrix\n",
    "    # here we use Gaussian random matrix ( \\mathcal{N}(0, 1/target_dim) )\n",
    "    # it means, R_ij ~ \\mathcal{N}(0, 1/target_dim)\n",
    "    R = np.random.randn(X.shape[1], target_dim) / np.sqrt(target_dim)\n",
    "    \n",
    "    # project the data matrix X onto the lower-dimensional space using R\n",
    "    Z = X @ R  # Z = XR\n",
    "\n",
    "    # optionally, normalize the projected data Z to preserve the Euclidean distance\n",
    "    Z = Z / np.sqrt(target_dim)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE: Utiliza la distribucion de probabilidad de la vecindad de los puntos para proyectar los datos en un espacio de menor dimension\n",
    "def tSNE(X: np.ndarray, target_dim, perplexity=30, max_iter=1000, learning_rate=0.1, toler=0.001):\n",
    "    # let perplexity  as \\sigma_i^2 for each point i\n",
    "    # for each pair of points i and j, compute the conditional probability p_{j|i}\n",
    "    # that x_j is the neighbor of x_i, using the Gaussian kernel\n",
    "    m = X.shape[0], n = X.shape[1]  # X in R^{m x n}\n",
    "\n",
    "    def compute_pairwise_probabilities(X, perplexity):\n",
    "        n = X.shape[0]\n",
    "        P = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            dists = np.linalg.norm(X - X[i], axis=1)\n",
    "            p = np.exp(-dists ** 2 / (2 * perplexity ** 2))\n",
    "            p[i] = 0  # set the self-probability to 0\n",
    "            p = p / p.sum()  # normalize the probabilities\n",
    "            P[i] = p\n",
    "        return P\n",
    "    \n",
    "    # compute the pairwise probabilities\n",
    "    P = compute_pairwise_probabilities(X, perplexity)\n",
    "\n",
    "    # symmetrize the probabilities to get the joint probabilities\n",
    "    P = (P + P.T) / (2 * m)\n",
    "\n",
    "    # initialize the low-dimensional representation Y \\in R^{m x target_dim} using Gaussian random noise\n",
    "    Y = np.random.randn(m, target_dim)\n",
    "\n",
    "    # compute the pairwise similarities in the low-dimensional space. omputing for each pair q_{ij} using the Student's t-distribution with one degree of freedom\n",
    "    def compute_pairwise_similarities(Y):\n",
    "        n = Y.shape[0]\n",
    "        Q = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            dists = np.linalg.norm(Y - Y[i], axis=1)\n",
    "            q = 1 / (1 + dists ** 2)\n",
    "            q[i] = 0  # set the self-similarity to 0\n",
    "            q = q / q.sum()  # normalize the similarities\n",
    "            Q[i] = q\n",
    "        return Q\n",
    "    \n",
    "    # compute the pairwise similarities in the low-dimensional space\n",
    "    Q = compute_pairwise_similarities(Y)\n",
    "    # Minimize the Kullback-Leibler divergence between P and Q\n",
    "    C = np.sum(P * np.log(P / Q))\n",
    "\n",
    "    # perfom the gradient descent to minimize the KL divergence\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        # compute the gradient of the KL divergence\n",
    "        dC = np.zeros((m, target_dim))\n",
    "        for i in range(m):\n",
    "            dC[i] = 4 * np.sum((P[i] - Q[i])[:, None] * (Y[i] - Y), axis=0)\n",
    "\n",
    "        # update the low-dimensional representation Y\n",
    "        Y = Y - learning_rate * dC\n",
    "\n",
    "        # compute the pairwise similarities in the low-dimensional space\n",
    "        Q = compute_pairwise_similarities(Y)\n",
    "        # compute the KL divergence\n",
    "        C_new = np.sum(P * np.log(P / Q))\n",
    "\n",
    "        # check the convergence\n",
    "        if np.abs(C - C_new) < toler:\n",
    "            break\n",
    "        C = C_new\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP: Es similar al t-SNE, utiliza la distribucion de probabilidad de la vecindad de los puntos para proyectar los datos en un espacio de menor dimension\n",
    "def UMAP(X: np.ndarray, target_dim, n_neighbors=15, min_dist=0.1, n_epochs=100, learning_rate=0.1, toler=0.001):\n",
    "    # compute the fuzzy simplicial set\n",
    "    # compute the k-nearest neighbors graph\n",
    "    k = n_neighbors\n",
    "    m = X.shape[0], n = X.shape[1]  # X in R^{m x n}\n",
    "\n",
    "    def compute_knn_graph(X, k):\n",
    "        n = X.shape[0]\n",
    "        G = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            dists = np.linalg.norm(X - X[i], axis=1)\n",
    "            idx = np.argsort(dists)[1:k+1]  # exclude the self-neighbor\n",
    "            G[i, idx] = 1\n",
    "        return G\n",
    "    \n",
    "    # compute the k-nearest neighbors graph\n",
    "    G = compute_knn_graph(X, k)\n",
    "\n",
    "    # compute the membership strength using the Gaussian kernel\n",
    "    def compute_membership_strength(X, G, min_dist):\n",
    "        n = X.shape[0]\n",
    "        A = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            idx = np.where(G[i] == 1)[0]\n",
    "            dists = np.linalg.norm(X[idx] - X[i], axis=1)\n",
    "            a = np.exp(-dists / min_dist)\n",
    "            A[i, idx] = a\n",
    "        return A\n",
    "    \n",
    "    # compute the membership strength\n",
    "    A = compute_membership_strength(X, G, min_dist)\n",
    "\n",
    "    # symmetrize the membership strength, wuer w^sym_{ij} = w_{ij} + w_{ji} - w_{ij} w_{ji}\n",
    "    A = A + A.T - A * A.T\n",
    "\n",
    "    # init the matrix Y\n",
    "    Y = np.random.randn(m, target_dim)\n",
    "\n",
    "    # Optimize the low-dimensional representation Y\n",
    "    for _ in range(n_epochs):\n",
    "        # compute the fuzzy simplicial set\n",
    "        # q_{ij} = (1 + ||y_i - y_j||^2)^-1\n",
    "        Q = np.zeros((m, m))\n",
    "        for i in range(m):\n",
    "            dists = np.linalg.norm(Y - Y[i], axis=1)\n",
    "            q = 1 / (1 + dists ** 2)\n",
    "            Q[i] = q\n",
    "        Q = Q / Q.sum()\n",
    "\n",
    "        # minimize the cross entropy between A and Q\n",
    "        C = np.sum(A * np.log(A / Q))\n",
    "\n",
    "        # use gradient descent to minimize the cross entropy\n",
    "        dC = np.zeros((m, target_dim))\n",
    "        for i in range(m):\n",
    "            dC[i] = 2 * np.sum(A[i][:, None] * (Y[i] - Y), axis=0)\n",
    "\n",
    "        # update the low-dimensional representation Y\n",
    "        Y = Y - learning_rate * dC\n",
    "        if np.linalg.norm(dC) < toler:\n",
    "            break\n",
    "\n",
    "    return Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
